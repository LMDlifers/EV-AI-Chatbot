{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2563eb; color: white; padding: 20px; border-radius: 8px; margin: 10px 0; max-width: 1120px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "\n",
    "## Dataset Overview: Electric Vehicle Charging Stations (2024)\n",
    "\n",
    "To power our AI chatbot, we are sourcing our knowledge from the **Electric Vehicle Charging Stations (2024)** dataset. This modern dataset is ideal for our mockup as it provides the essential data points needed to answer specific user queries. \n",
    "\n",
    "### Key Features:\n",
    "- **Rich location information** like Latitude and Longitude for proximity searches\n",
    "- **EV Connector Types** and **EV Network data** for filtering compatible and preferred charging options\n",
    "- Comprehensive coverage of charging stations across the network\n",
    "\n",
    "We will load this data into a **pandas DataFrame**, which will serve as the core \"database\" for all bot-driven station lookups.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 1)) (6.30.1)\n",
      "Requirement already satisfied: kagglehub in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (0.3.13)\n",
      "Collecting sentence-transformers (from -r ../requirements.txt (line 4))\n",
      "  Using cached sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting scikit-learn (from -r ../requirements.txt (line 5))\n",
      "  Using cached scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (2.3.2)\n",
      "Requirement already satisfied: appnope>=0.1.2 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (9.5.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (27.0.2)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipykernel->-r ../requirements.txt (line 1)) (5.14.3)\n",
      "Requirement already satisfied: pyyaml in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from kagglehub->-r ../requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from kagglehub->-r ../requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from kagglehub->-r ../requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: pandas in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from kagglehub[pandas-datasets]->-r ../requirements.txt (line 3)) (2.3.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scipy (from sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Pillow (from sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r ../requirements.txt (line 5))\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r ../requirements.txt (line 5))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: decorator in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from jupyter-client>=8.0.0->ipykernel->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r ../requirements.txt (line 1)) (4.4.0)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached regex-2025.7.34-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from pandas->kagglehub[pandas-datasets]->-r ../requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from pandas->kagglehub[pandas-datasets]->-r ../requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from requests->kagglehub->-r ../requirements.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from requests->kagglehub->-r ../requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from requests->kagglehub->-r ../requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from requests->kagglehub->-r ../requirements.txt (line 2)) (2025.8.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers->-r ../requirements.txt (line 4))\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/kootony/Documents/GitHub/EV-AI-Chatbot/venv/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 1)) (0.2.3)\n",
      "Using cached sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Using cached scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl (20.8 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2025.7.34-cp313-cp313-macosx_11_0_arm64.whl (285 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing_extensions, threadpoolctl, sympy, setuptools, scipy, safetensors, regex, Pillow, networkx, MarkupSafe, joblib, hf-xet, fsspec, filelock, scikit-learn, jinja2, huggingface-hub, torch, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.3.0 filelock-3.19.1 fsspec-2025.7.0 hf-xet-1.1.9 huggingface-hub-0.34.4 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 regex-2025.7.34 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 setuptools-80.9.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.0 torch-2.8.0 transformers-4.56.0 typing_extensions-4.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVChargingStationBot:\n",
    "    \"\"\"\n",
    "    AI Chatbot for Electric Vehicle Charging Station queries.\n",
    "    Supports semantic search, geographic clustering, and structured filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_handle: str = \"sahirmaharajj/electric-vehicle-charging-stations-2024\", \n",
    "                 file_path: str = None, embedding_model: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the EV Charging Station Bot.\n",
    "        \n",
    "        Args:\n",
    "            dataset_handle: Kaggle dataset identifier\n",
    "            file_path: Specific file to load from dataset\n",
    "            embedding_model: SentenceTransformer model for semantic search\n",
    "        \"\"\"\n",
    "        self.dataset_handle = dataset_handle\n",
    "        self.file_path = file_path\n",
    "        self.embedding_model_name = embedding_model\n",
    "        \n",
    "        # Core data structures\n",
    "        self.df = None\n",
    "        self.model = None\n",
    "        self.station_embeddings = None\n",
    "        self.geo_clusters = None\n",
    "        \n",
    "        # Initialize the bot\n",
    "        self._load_data()\n",
    "        self._preprocess_data()\n",
    "        self._initialize_embeddings()\n",
    "        \n",
    "    def _load_data(self) -> None:\n",
    "        \"\"\"Load the EV charging station dataset.\"\"\"\n",
    "        try:\n",
    "            if self.file_path:\n",
    "                self.df = kagglehub.dataset_load(\n",
    "                    KaggleDatasetAdapter.PANDAS,\n",
    "                    self.dataset_handle,\n",
    "                    self.file_path\n",
    "                )\n",
    "            else:\n",
    "                # Download and explore dataset first\n",
    "                path = kagglehub.dataset_download(self.dataset_handle)\n",
    "                print(f\"Dataset downloaded to: {path}\")\n",
    "                # User needs to specify the correct file_path\n",
    "                raise ValueError(\"Please specify the file_path after exploring the dataset\")\n",
    "                \n",
    "            print(f\"Loaded {len(self.df)} charging stations\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _preprocess_data(self) -> None:\n",
    "        \"\"\"Preprocess the dataset for optimal searching.\"\"\"\n",
    "        # Clean and standardize columns FIRST\n",
    "        self._clean_data()\n",
    "        \n",
    "        # Extract coordinates from georeferenced column\n",
    "        if 'New Georeferenced Column' in self.df.columns:\n",
    "            coordinates = self.df['New Georeferenced Column'].str.extract(r'POINT \\(([^)]+)\\)')\n",
    "            coord_split = coordinates[0].str.split(' ', expand=True)\n",
    "            if len(coord_split.columns) >= 2:\n",
    "                self.df['longitude'] = pd.to_numeric(coord_split[0], errors='coerce')\n",
    "                self.df['latitude'] = pd.to_numeric(coord_split[1], errors='coerce')\n",
    "        \n",
    "        # Create rich descriptions for semantic search (after cleaning)\n",
    "        self.df['description'] = self.df.apply(self._create_station_description, axis=1)\n",
    "        \n",
    "        print(\"Data preprocessing completed\")\n",
    "    \n",
    "    def _create_station_description(self, row) -> str:\n",
    "        \"\"\"Create a rich text description for each charging station.\"\"\"\n",
    "        description_parts = []\n",
    "        \n",
    "        if pd.notna(row.get('Station Name')):\n",
    "            description_parts.append(f\"Station: {row['Station Name']}\")\n",
    "        \n",
    "        if pd.notna(row.get('City')) and pd.notna(row.get('Street Address')):\n",
    "            description_parts.append(f\"Located in {row['City']} at {row['Street Address']}\")\n",
    "        \n",
    "        if pd.notna(row.get('Access Days Time')):\n",
    "            description_parts.append(f\"Hours: {row['Access Days Time']}\")\n",
    "        \n",
    "        # Charging capabilities\n",
    "        charging_info = []\n",
    "        if pd.notna(row.get('EV Level1 EVSE Num')) and row['EV Level1 EVSE Num'] > 0:\n",
    "            charging_info.append(f\"Level 1: {row['EV Level1 EVSE Num']} ports\")\n",
    "        if pd.notna(row.get('EV Level2 EVSE Num')) and row['EV Level2 EVSE Num'] > 0:\n",
    "            charging_info.append(f\"Level 2: {row['EV Level2 EVSE Num']} ports\")\n",
    "        if pd.notna(row.get('EV DC Fast Count')) and row['EV DC Fast Count'] > 0:\n",
    "            charging_info.append(f\"DC Fast: {row['EV DC Fast Count']} ports\")\n",
    "        \n",
    "        if charging_info:\n",
    "            description_parts.append(\"Charging: \" + \", \".join(charging_info))\n",
    "        \n",
    "        return \". \".join(description_parts)\n",
    "    \n",
    "    def _clean_data(self) -> None:\n",
    "        \"\"\"Clean and standardize the dataset.\"\"\"\n",
    "        # Convert numeric columns - handle \"NONE\" strings\n",
    "        numeric_cols = ['EV Level1 EVSE Num', 'EV Level2 EVSE Num', 'EV DC Fast Count']\n",
    "        for col in numeric_cols:\n",
    "            if col in self.df.columns:\n",
    "                # Replace \"NONE\" with 0, then convert to numeric\n",
    "                self.df[col] = self.df[col].astype(str).str.replace('NONE', '0')\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce').fillna(0).astype(int)\n",
    "        \n",
    "        # Clean string columns\n",
    "        string_cols = ['Station Name', 'City', 'Street Address']\n",
    "        for col in string_cols:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].astype(str).str.strip()\n",
    "    \n",
    "    def _initialize_embeddings(self) -> None:\n",
    "        \"\"\"Initialize the sentence transformer model and create embeddings.\"\"\"\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.embedding_model_name)\n",
    "            self.station_embeddings = self.model.encode(self.df['description'].tolist())\n",
    "            print(\"Embeddings initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not initialize embeddings: {e}\")\n",
    "            self.model = None\n",
    "            self.station_embeddings = None\n",
    "    \n",
    "    def setup_geographic_clustering(self, n_clusters: int = 20) -> None:\n",
    "        \"\"\"Setup geographic clustering for location-based optimization.\"\"\"\n",
    "        if 'latitude' in self.df.columns and 'longitude' in self.df.columns:\n",
    "            # Remove rows with missing coordinates\n",
    "            valid_coords = self.df.dropna(subset=['latitude', 'longitude'])\n",
    "            \n",
    "            if len(valid_coords) > 0:\n",
    "                kmeans = KMeans(n_clusters=min(n_clusters, len(valid_coords)), random_state=42)\n",
    "                coords = valid_coords[['latitude', 'longitude']].values\n",
    "                cluster_labels = kmeans.fit_predict(coords)\n",
    "                \n",
    "                # Map clusters back to original dataframe\n",
    "                self.df['geo_cluster'] = -1  # Default for missing coordinates\n",
    "                self.df.loc[valid_coords.index, 'geo_cluster'] = cluster_labels\n",
    "                \n",
    "                self.geo_clusters = kmeans\n",
    "                print(f\"Geographic clustering completed with {n_clusters} clusters\")\n",
    "            else:\n",
    "                print(\"No valid coordinates found for clustering\")\n",
    "        else:\n",
    "            print(\"Latitude/Longitude columns not available for clustering\")\n",
    "    \n",
    "    # === SEARCH METHODS ===\n",
    "    def find_by_city(self, city: str, limit: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Find charging stations by city name.\"\"\"\n",
    "        mask = self.df['City'].str.contains(city, case=False, na=False)\n",
    "        return self.df[mask].head(limit)\n",
    "    \n",
    "    def find_fast_charging(self, city: str = None, limit: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Find stations with DC fast charging.\"\"\"\n",
    "        # Convert to numeric on the fly if needed\n",
    "        dc_fast_col = pd.to_numeric(self.df['EV DC Fast Count'], errors='coerce').fillna(0)\n",
    "        mask = dc_fast_col > 0\n",
    "        \n",
    "        if city:\n",
    "            city_mask = self.df['City'].str.contains(city, case=False, na=False)\n",
    "            mask = mask & city_mask\n",
    "        \n",
    "        return self.df[mask].head(limit)\n",
    "\n",
    "    def find_level2_charging(self, city: str = None, min_ports: int = 1, limit: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Find stations with Level 2 charging.\"\"\"\n",
    "        # Convert to numeric on the fly if needed\n",
    "        level2_col = pd.to_numeric(self.df['EV Level2 EVSE Num'], errors='coerce').fillna(0)\n",
    "        mask = level2_col >= min_ports\n",
    "        \n",
    "        if city:\n",
    "            city_mask = self.df['City'].str.contains(city, case=False, na=False)\n",
    "            mask = mask & city_mask\n",
    "        \n",
    "        return self.df[mask].head(limit)\n",
    "    \n",
    "    def find_24_hour_stations(self, city: str = None, limit: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Find 24-hour accessible charging stations.\"\"\"\n",
    "        mask = self.df['Access Days Time'].str.contains('24 hours', case=False, na=False)\n",
    "        \n",
    "        if city:\n",
    "            city_mask = self.df['City'].str.contains(city, case=False, na=False)\n",
    "            mask = mask & city_mask\n",
    "        \n",
    "        return self.df[mask].head(limit)\n",
    "    \n",
    "    def find_nearby_stations(self, lat: float, lon: float, radius_km: float = 10, limit: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Find charging stations within a radius of given coordinates.\"\"\"\n",
    "        if 'latitude' not in self.df.columns or 'longitude' not in self.df.columns:\n",
    "            raise ValueError(\"Coordinate data not available\")\n",
    "        \n",
    "        # Calculate distances using Haversine formula\n",
    "        distances = self._calculate_distances(lat, lon)\n",
    "        nearby_mask = distances <= radius_km\n",
    "        \n",
    "        # Sort by distance\n",
    "        nearby_stations = self.df[nearby_mask].copy()\n",
    "        nearby_stations['distance_km'] = distances[nearby_mask]\n",
    "        \n",
    "        return nearby_stations.sort_values('distance_km').head(limit)\n",
    "    \n",
    "    def semantic_search(self, query: str, limit: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Perform semantic search using embeddings.\"\"\"\n",
    "        if self.model is None or self.station_embeddings is None:\n",
    "            raise ValueError(\"Embeddings not available. Initialize embeddings first.\")\n",
    "        \n",
    "        # Encode the query\n",
    "        query_embedding = self.model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.station_embeddings)[0]\n",
    "        \n",
    "        # Get top results\n",
    "        top_indices = np.argsort(similarities)[::-1][:limit]\n",
    "        results = self.df.iloc[top_indices].copy()\n",
    "        results['similarity_score'] = similarities[top_indices]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_distances(self, lat: float, lon: float) -> np.ndarray:\n",
    "        \"\"\"Calculate distances using Haversine formula.\"\"\"\n",
    "        R = 6371  # Earth's radius in kilometers\n",
    "        \n",
    "        lat_rad = np.radians(lat)\n",
    "        lon_rad = np.radians(lon)\n",
    "        \n",
    "        station_lats = np.radians(self.df['latitude'].fillna(0))\n",
    "        station_lons = np.radians(self.df['longitude'].fillna(0))\n",
    "        \n",
    "        dlat = station_lats - lat_rad\n",
    "        dlon = station_lons - lon_rad\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat_rad) * np.cos(station_lats) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        \n",
    "        return R * c\n",
    "    \n",
    "    def get_station_details(self, station_id: int) -> Dict:\n",
    "        \"\"\"Get detailed information about a specific station.\"\"\"\n",
    "        station = self.df.iloc[station_id]\n",
    "        return station.to_dict()\n",
    "    \n",
    "    def get_cities(self) -> List[str]:\n",
    "        \"\"\"Get list of all cities with charging stations.\"\"\"\n",
    "        return sorted(self.df['City'].unique())\n",
    "    \n",
    "    def get_summary_stats(self) -> Dict:\n",
    "        \"\"\"Get summary statistics about the dataset.\"\"\"\n",
    "        return {\n",
    "            'total_stations': len(self.df),\n",
    "            'cities': len(self.df['City'].unique()),\n",
    "            'level1_stations': (self.df['EV Level1 EVSE Num'] > 0).sum(),\n",
    "            'level2_stations': (self.df['EV Level2 EVSE Num'] > 0).sum(),\n",
    "            'fast_dc_stations': (self.df['EV DC Fast Count'] > 0).sum(),\n",
    "            'total_level2_ports': self.df['EV Level2 EVSE Num'].sum(),\n",
    "            'total_fast_dc_ports': self.df['EV DC Fast Count'].sum()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 385 charging stations\n",
      "Data preprocessing completed\n",
      "Embeddings initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "bot = EVChargingStationBot(file_path=\"Electric_Vehicle_Charging_Stations.csv\")\n",
    "results = bot.find_fast_charging(\"Boston\")\n",
    "nearby = bot.find_nearby_stations(42.3601, -71.0589, radius_km=5)\n",
    "semantic_results = bot.semantic_search(\"Tesla supercharger downtown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station Name</th>\n",
       "      <th>Street Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Access Days Time</th>\n",
       "      <th>EV Level1 EVSE Num</th>\n",
       "      <th>EV Level2 EVSE Num</th>\n",
       "      <th>EV DC Fast Count</th>\n",
       "      <th>EV Other Info</th>\n",
       "      <th>New Georeferenced Column</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>description</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Ridgeway Shopping Center - Tesla Supercharger</td>\n",
       "      <td>2233 Summer Street</td>\n",
       "      <td>Stamford</td>\n",
       "      <td>24 hours daily; for Tesla use only</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NONE</td>\n",
       "      <td>POINT (-73.546435 41.068704)</td>\n",
       "      <td>-73.546435</td>\n",
       "      <td>41.068704</td>\n",
       "      <td>Station: Ridgeway Shopping Center - Tesla Supe...</td>\n",
       "      <td>0.763580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Plaza at Buckland Hills - Tesla Supercharger</td>\n",
       "      <td>1470 Pleasant Valley Road</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>24 hours daily; for Tesla use only</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>NONE</td>\n",
       "      <td>POINT (-72.562282 41.80452)</td>\n",
       "      <td>-72.562282</td>\n",
       "      <td>41.804520</td>\n",
       "      <td>Station: The Plaza at Buckland Hills - Tesla S...</td>\n",
       "      <td>0.749950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Greenwich Northbound Travel Plaza - Tesla Supe...</td>\n",
       "      <td>3000 Merritt Parkway</td>\n",
       "      <td>Greenwich</td>\n",
       "      <td>24 hours daily; for Tesla use only</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NONE</td>\n",
       "      <td>POINT (-73.671661 41.041538)</td>\n",
       "      <td>-73.671661</td>\n",
       "      <td>41.041538</td>\n",
       "      <td>Station: Greenwich Northbound Travel Plaza - T...</td>\n",
       "      <td>0.746748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Grand Central Fashion Plaza Shopping Center - ...</td>\n",
       "      <td>1145 High Ridge Rd</td>\n",
       "      <td>North Stamford</td>\n",
       "      <td>24 hours daily; for Tesla use only</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NONE</td>\n",
       "      <td>POINT (-73.546513 41.107722)</td>\n",
       "      <td>-73.546513</td>\n",
       "      <td>41.107722</td>\n",
       "      <td>Station: Grand Central Fashion Plaza Shopping ...</td>\n",
       "      <td>0.739274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Greenwich Southbound Travel Plaza - Tesla Supe...</td>\n",
       "      <td>2000 Merritt Parkway</td>\n",
       "      <td>Greenwich</td>\n",
       "      <td>24 hours daily; for Tesla use only</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NONE</td>\n",
       "      <td>POINT (-73.673445 41.040555)</td>\n",
       "      <td>-73.673445</td>\n",
       "      <td>41.040555</td>\n",
       "      <td>Station: Greenwich Southbound Travel Plaza - T...</td>\n",
       "      <td>0.737158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Station Name  \\\n",
       "280      Ridgeway Shopping Center - Tesla Supercharger   \n",
       "18    The Plaza at Buckland Hills - Tesla Supercharger   \n",
       "217  Greenwich Northbound Travel Plaza - Tesla Supe...   \n",
       "375  Grand Central Fashion Plaza Shopping Center - ...   \n",
       "160  Greenwich Southbound Travel Plaza - Tesla Supe...   \n",
       "\n",
       "                Street Address            City  \\\n",
       "280         2233 Summer Street        Stamford   \n",
       "18   1470 Pleasant Valley Road      Manchester   \n",
       "217       3000 Merritt Parkway       Greenwich   \n",
       "375         1145 High Ridge Rd  North Stamford   \n",
       "160       2000 Merritt Parkway       Greenwich   \n",
       "\n",
       "                       Access Days Time  EV Level1 EVSE Num  \\\n",
       "280  24 hours daily; for Tesla use only                   0   \n",
       "18   24 hours daily; for Tesla use only                   0   \n",
       "217  24 hours daily; for Tesla use only                   0   \n",
       "375  24 hours daily; for Tesla use only                   0   \n",
       "160  24 hours daily; for Tesla use only                   0   \n",
       "\n",
       "     EV Level2 EVSE Num  EV DC Fast Count EV Other Info  \\\n",
       "280                   0                12          NONE   \n",
       "18                    0                16          NONE   \n",
       "217                   0                 4          NONE   \n",
       "375                   0                 8          NONE   \n",
       "160                   0                 4          NONE   \n",
       "\n",
       "         New Georeferenced Column  longitude   latitude  \\\n",
       "280  POINT (-73.546435 41.068704) -73.546435  41.068704   \n",
       "18    POINT (-72.562282 41.80452) -72.562282  41.804520   \n",
       "217  POINT (-73.671661 41.041538) -73.671661  41.041538   \n",
       "375  POINT (-73.546513 41.107722) -73.546513  41.107722   \n",
       "160  POINT (-73.673445 41.040555) -73.673445  41.040555   \n",
       "\n",
       "                                           description  similarity_score  \n",
       "280  Station: Ridgeway Shopping Center - Tesla Supe...          0.763580  \n",
       "18   Station: The Plaza at Buckland Hills - Tesla S...          0.749950  \n",
       "217  Station: Greenwich Northbound Travel Plaza - T...          0.746748  \n",
       "375  Station: Grand Central Fashion Plaza Shopping ...          0.739274  \n",
       "160  Station: Greenwich Southbound Travel Plaza - T...          0.737158  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a decoder to return the results with a direction to it\n",
    "semantic_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
